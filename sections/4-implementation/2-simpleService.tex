\subsection{Implementing a test Service} \label{sec:testService}
To test the Kubernetes setup I developed a simple go microservice called \textit{hello}. It listens on port 3000 and has three endpoints \textit{/hello/}, \textit{/hello/info} and \textit{/hello/path}. They don't implement any deeper logic but pose as an exemplary implementation for future services. I show exemplary Kubernetes configuration files and use this section to explain their meaning in more detail. For easier testability with cURL this microservice is based on json and not on protobufs.\\
\Cref{lst:dockerfileHello} shows the Dockerfile for building the microservice.
\input{sections/4-implementation/codeSnippets/dockerfileHello.tex}
The code is cross-compiled in a builder container, which provides an isolated and replaceable build environment, this happens in line 1--8. Line 1 takes a fresh go alpine container with the latest go version. I then expose the port 3000 for the application to interact with its environment and copy the source code in the new container, line 2 and 3, respectively. Each new command produces a new temporary cached container, thus I the lines, 4--8, are all one \textit{RUN} command. It first adds a new user called \textit{scratchuser} without root privileges and the goes into the go main file directory and cross-compiles the application for ARMv7 (the last step is line 6--8). In lines 10--15, the final image is build. Line 10 initializes a new scratch container. This image is empty and thus uses a less memory and system resources. In lines 11--13 the relevant documents are copied in the new container. This includes the certificates from certificate authorities\footnote{They are important for encrypted connections.} (11), the application binary (12) and the user password (13). Line 14 switches to the scratchuser. This user has no root privileges and thus can never gain system control. Lastly, I specify the command run when a container based on the build image is run (15).\\
To see how important it is to use a scratch image as a base for memory purposes, consider \cref{fig:imageSizeComparison}. It shows just how much can be saved using this technique.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{figures/imageSizeComparison.png}
    \caption{The image sizes of the application.}
    \label{fig:imageSizeComparison}
\end{figure}
The go alpine image at the bottom is 350MB, the final image is a hefty 28MB bigger at 378MB (lines 1--8 in \cref{lst:dockerfileHello}). Copying the binary, the user files and the  certificates into a scratch image takes up only 6.76MB in total (lines 11--13). That is significantly smaller and with less overhead. When the container runs, only the binary is loaded and nothing else is running in the background. Contrast that to the alpine image where every time it is run, it starts a shell command line and includes an entire go build environment. The image is then tagged with a repository and tag name and pushed to the corresponding repository.\\
On the Kubernetes master I defined a service for the \textit{hello} application with the manifest shown in \cref{lst:serviceManifest}. It is an abstraction defining the guidelines how to access the pods inside a service, as the pods themselves are volatile\footnote{Each pod has a unique IP but Kubernetes does not guarantee for Pods and can reschedule pods at any moment}. It thus enables decoupling of the networking and the actual application from an outsiders perspective.
\input{sections/4-implementation/codeSnippets/serviceManifest.tex}
Line 5 tells the master that the service should be deployed in the namespace \textit{hello-namespace}. Line 6 specifies that each pod of this service should be accessible on the node it is scheduled on without going through the Kubernetes ingress resource. This is called nodeport and specified in line 12 to be \textit{30001}. Line 7 tells Kubernetes that the application corresponding to the service is called \textit{hello}. Finally, line 13 and 14 specify the ports the actual container expose.\\
For the actual state description of the application I use a \textit{Deployment} shown in \cref{lst:deploymentManifest}. A deployment specifies the desired state and the Deployment controller changes the actual state inside the cluster towards the desired state. 
\input{sections/4-implementation/codeSnippets/deploymentManifest.tex}
Line 3 and 4 specify the name and namespace of the deployment. Lines 17--34 specify the pod affinities, which places a constrained on where a certain pod can be scheduled. Similarly, anti-affinities constrain a pod to where can not be scheduled\footnote{I deployed another pod to the node beforehand with the correct label}. Lines 18--25 specify a node affinity "it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node"\cite{affinitiesKubernetes:online}.
Lines 26--34 specify the pod affinity. Pods of the deployment will only be scheduled on nodes containing a pod with the specified label. This enables orchestration based on other pods. Lines 35 and 36 specify the single node a deployment should be scheduled on. These three methods can be used together but have to chosen carefully. Finally, line 37--39 specify the container which should be deployed inside a pod (the port selection is hidden).\\
With this configuration it is possible to clearly specify the nodes a deployment should be scheduled on. I omitted how to accomplish it via namespace, which has many benefits to it as well. However, deployments, the resource type used in this section, are volatile deployments and should thus only be used for stateless application, like the \textit{hello} application. Stateful pods require the resource type \textit{StatefulSet} and pods supposed to run on every node require the resource type \textit{ReplicaSet}, see \cref{sec:statefulvsdeploymentvsBLAAAA} for more information.
